{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Filipino-ULMFiT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YivYGXqKuBCO",
        "Xeq8PeNSvU_2",
        "swgJLIMAwOkJ"
      ],
      "authorship_tag": "ABX9TyOOJKCLmAdMnk226FNwOUTk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danjohnvelasco/Filipino-ULMFiT/blob/master/Filipino_ULMFiT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0wRH746s7Ns"
      },
      "source": [
        "# Filipino ULMFiT\n",
        "This notebook shows the ULMFiT approach to Filipino text classification task on [Hate Speech Dataset](https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks#datasets) using a pre-trained Filipino language model. To learn more about the ULMFiT approach check the [Docs](https://docs.fast.ai/tutorial.text) and [Paper](https://arxiv.org/abs/1801.06146).\n",
        "\n",
        "Originally posted in this [repository](https://github.com/danjohnvelasco/Filipino-ULMFiT)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvh9xdr2sak0"
      },
      "source": [
        "# Install fastai v2\n",
        "# uncomment if your environment doesn't use fastai >= v2.\n",
        "# run pip freeze to check if fastai is installed\n",
        "\n",
        "# !pip install -U fastai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l98GbAyrss4r"
      },
      "source": [
        "# if you're on Colab, make sure you're using a GPU instance.\n",
        "# Make sure that your GPU supports mixed-precisoin traning (e.g. Tesla T4, P4, P100, V100)\n",
        "# !nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YivYGXqKuBCO"
      },
      "source": [
        "# Before you start...\n",
        "\n",
        "1.  Import dependencies\n",
        "2.  Define convenient functions for later use\n",
        "3.  Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ8NkN3ks09j"
      },
      "source": [
        "from fastai.text.all import *\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M-m2zJ0u0Eh"
      },
      "source": [
        "# Run this function before creating a learner if you want  \n",
        "# your work to be reproducible\n",
        "\n",
        "# Convenience function for setting the random seed manually\n",
        "def set_random_seed(seed):\n",
        "    # python RNG\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "\n",
        "    # pytorch RNGs\n",
        "    import torch\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # numpy RNG\n",
        "    import numpy as np\n",
        "    np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6X9FthozhBj"
      },
      "source": [
        "# Convenience function for testing model on test set\n",
        "def get_test_acc(learn):\n",
        "    # Create dataloader for test set\n",
        "    test_dl = learn.dls.test_dl(test_df, with_label=True)\n",
        "    # Make predictions on test set\n",
        "    pred_probas, _ , pred_classes = learn.get_preds(dl=test_dl, with_decoded=True) \n",
        "    # get accuracy of (y_true, y_pred)\n",
        "    return accuracy_score(test_df.label.values, pred_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLDggOHht90q"
      },
      "source": [
        "# Modify this to match your data directory\n",
        "train_df = pd.read_csv(\"train.csv\", lineterminator='\\n')\n",
        "valid_df = pd.read_csv(\"valid.csv\", lineterminator='\\n')\n",
        "test_df = pd.read_csv(\"test.csv\", lineterminator='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IIPPtSeuiIl"
      },
      "source": [
        "# add 'is_valid' column (for fastai train-val splitting)\n",
        "valid_df['is_valid'] = True\n",
        "test_df['is_valid'] = False\n",
        "train_df['is_valid'] = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qigAyT6pvgWR"
      },
      "source": [
        "# Concatenate train and validation set\n",
        "lm_df_10k =  pd.concat([train_df, valid_df])\n",
        "lm_df_10k.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hk_8685up9-"
      },
      "source": [
        "# HYPERPARAMETERS\n",
        "lr = 5e-2\n",
        "wd = 0.1\n",
        "moms = (0.8,0.7,0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvFNeVqYvK9P"
      },
      "source": [
        "# Filenames of pre-trained LM weights and vocab\n",
        "pretrained_fnames = ['finetuned_weights_20', 'vocab']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xeq8PeNSvU_2"
      },
      "source": [
        "# Language Model Fine-tuning\n",
        "\n",
        "Here, we fine-tune the pre-trained language model to better learn the vocab and syntax of the target corpus which is, in our case, the Hate Speech Dataset.\n",
        "\n",
        "**About the pre-trained language model files:**\n",
        "\n",
        "By default, fastai looks for models inside the 'models' folder. Make sure that the pre-trained models and vocab are in 'models' folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVQCKuYNvWIE"
      },
      "source": [
        "# Create a dataloader for language model fine-tuning\n",
        "dls_lm = DataBlock(blocks=TextBlock.from_df('text', is_lm=True),\n",
        "                    get_x=ColReader('text'),\n",
        "                    splitter=ColSplitter())\n",
        "                    .dataloaders(lm_df_10k, bs=128, seq_len=72, min_freq=2, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVtS9TNez9mg"
      },
      "source": [
        "# Uncomment this if you want early stopping and save your best model.\n",
        "# This is fastai callbacks, see docs for more info\n",
        "cbs = [EarlyStoppingCallback(monitor='valid_loss', patience=2), SaveModelCallback()]\n",
        "\n",
        "# Notice the pretrained_fnames parameter.\n",
        "# Here we pass the list of filenames of pretrained weights and vocab.pkl\n",
        "# This is where the use of pre-trained language model happens.\n",
        "learn = language_model_learner(\n",
        "    dls_lm, AWD_LSTM, drop_mult=0.5, \n",
        "    metrics=[accuracy, Perplexity()],\n",
        "    pretrained_fnames=pretrained_fnames,\n",
        "    cbs=cbs).to_fp16()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L92DgUuv0S3Q"
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rihr5Xw0TQB"
      },
      "source": [
        "# train last layers first\n",
        "learn.fit_one_cycle(1, 4e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7hXL_FL0Xht"
      },
      "source": [
        "# train the whole network with smaller learning rate\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(6, 4e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTSmWpKrz3Xq"
      },
      "source": [
        "# Save encoder. To be used by text classifier learner.\n",
        "learn.save_encoder('lm_finetune_final_enc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txS6EOJW0m3V"
      },
      "source": [
        "# This is totally unrelated to training process but just for fun...\n",
        "# you can try generating text with the language model here\n",
        "TEXT = \"Ako ay\"\n",
        "N_WORDS = 40\n",
        "N_SENTENCES = 2\n",
        "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n",
        "         for _ in range(N_SENTENCES)]\n",
        "\n",
        "print(\"\\n\".join(preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swgJLIMAwOkJ"
      },
      "source": [
        "# Text Classifier Fine-tuning\n",
        "\n",
        "Here, we use the encoder of the fine-tuned language model to transfer the learnings to the classifier model. The model will learn to classify the text to binary labels, hate (1) or non-hate (0).\n",
        "\n",
        "Here, we'll apply gradual unfreezing and discriminative learning rates as discussed by [(Howard and Ruder, 2018)](https://arxiv.org/abs/1801.06146)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9ufQ2u6wQmP"
      },
      "source": [
        "# Create a dataloader for text classifier fine-tuning\n",
        "dls_clas = DataBlock(blocks=(TextBlock.from_df('text', seq_len=72, min_freq=2, vocab=dls_lm.vocab), CategoryBlock),\n",
        "                      get_x=ColReader('text'),\n",
        "                      get_y=ColReader('label'),\n",
        "                      splitter=ColSplitter()\n",
        "                      ).dataloaders(clas_df_10k, bs=128, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUmhI3xZx0jH"
      },
      "source": [
        "# Create learner\n",
        "learn = text_classifier_learner(dl, AWD_LSTM, moms=moms, wd=wd, metrics=accuracy).to_fp16()\n",
        "\n",
        "# Load encoder\n",
        "learn.load_encoder('lm_finetune_final_enc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP1vrjb3yrDf"
      },
      "source": [
        "# Train the last layers\n",
        "learn.fit_one_cycle(4, lr) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD2eS4EGzM8D"
      },
      "source": [
        "learn.freeze_to(-2) # Unfreeze a little bit\n",
        "learn.fit_one_cycle(2, slice(lr/(2.6**4),lr)) # Decrease the learning rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX46acsmzPNA"
      },
      "source": [
        "learn.freeze_to(-3) # Unfreeze a little bit more\n",
        "learn.fit_one_cycle(2, slice(lr/2/(2.6**4),lr/2)) # Decrease the learning rate more"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlZeBs_TzSof"
      },
      "source": [
        "learn.unfreeze() # Unfreeze the whole network\n",
        "learn.fit_one_cycle(1, slice(lr/10/(2.6**4),lr/10)) # Train the whole network with really small learning rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_pxmGaCzcFh"
      },
      "source": [
        "# Get accuracy with test set\n",
        "acc = get_test_acc(learn)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}